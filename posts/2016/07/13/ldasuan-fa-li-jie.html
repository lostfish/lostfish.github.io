<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8"> 
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="lostfish" />
        <meta name="copyright" content="lostfish" />

        <meta name="description" content="引言 LDA相关的资料已经非常丰富，比较好的如[1], [2], [3], [4] 和 [5]。 [1]是一篇中文博文，核心的点都提到了，基本参考了[2], [3], [4]的内容。 [2]是一篇科普佳作，结合数学史，把LDA涉及的很多函数及背后的数据原理通俗地讲了一遍，适合慢慢品读。 由于原始的LDA训练很慢，[3]主要讲了LDA的并行化实现，但是文献对LDA使用Gibbs采样涉及的公式进行了严密的推导，适合了解算法推导的每个环节。 [4]是较早一篇比较详细讲解LDA的论文，基本上[1], [2], [3]都参考了该文献, 并且GibbsLDA++ 的实现，也是主要参考该论文。 [5]讲述了 ...
" />
        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="lda, 主题模型, Tech, " />

<meta property="og:title" content="LDA算法理解 "/>
<meta property="og:url" content="/posts/2016/07/13/ldasuan-fa-li-jie.html" />
<meta property="og:description" content="引言 LDA相关的资料已经非常丰富，比较好的如[1], [2], [3], [4] 和 [5]。 [1]是一篇中文博文，核心的点都提到了，基本参考了[2], [3], [4]的内容。 [2]是一篇科普佳作，结合数学史，把LDA涉及的很多函数及背后的数据原理通俗地讲了一遍，适合慢慢品读。 由于原始的LDA训练很慢，[3]主要讲了LDA的并行化实现，但是文献对LDA使用Gibbs采样涉及的公式进行了严密的推导，适合了解算法推导的每个环节。 [4]是较早一篇比较详细讲解LDA的论文，基本上[1], [2], [3]都参考了该文献, 并且GibbsLDA++ 的实现，也是主要参考该论文。 [5]讲述了 ..." />
<meta property="og:site_name" content="当你老了" />
<meta property="og:article:author" content="lostfish" />
<meta property="og:article:published_time" content="2016-07-13T00:00:00+08:00" />
<meta name="twitter:title" content="LDA算法理解 ">
<meta name="twitter:description" content="引言 LDA相关的资料已经非常丰富，比较好的如[1], [2], [3], [4] 和 [5]。 [1]是一篇中文博文，核心的点都提到了，基本参考了[2], [3], [4]的内容。 [2]是一篇科普佳作，结合数学史，把LDA涉及的很多函数及背后的数据原理通俗地讲了一遍，适合慢慢品读。 由于原始的LDA训练很慢，[3]主要讲了LDA的并行化实现，但是文献对LDA使用Gibbs采样涉及的公式进行了严密的推导，适合了解算法推导的每个环节。 [4]是较早一篇比较详细讲解LDA的论文，基本上[1], [2], [3]都参考了该文献, 并且GibbsLDA++ 的实现，也是主要参考该论文。 [5]讲述了 ...">

        <title>LDA算法理解  · 当你老了
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="/theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/custom.css" media="screen">
        <link rel="shortcut icon" href="/theme/images/favicon.ico" type="image/x-icon" type="image/png" />
        <link rel="icon" href="/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" href="/theme/images/apple-touch-icon.png"  type="image/png" />
        <link rel="apple-touch-icon" sizes="57x57" href="/theme/images/apple-touch-icon-57x57.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="72x72" href="/theme/images/apple-touch-icon-72x72.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="76x76" href="/theme/images/apple-touch-icon-76x76.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="114x114" href="/theme/images/apple-touch-icon-114x114.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="120x120" href="/theme/images/apple-touch-icon-120x120.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="144x144" href="/theme/images/apple-touch-icon-144x144.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="当你老了 - Full Atom Feed" />
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
     })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-58905600-1', 'auto');
    ga('send', 'pageview');
</script>
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container-fluid">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="/"><span class=site-name>当你老了</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="">Home</a></li>
                            <li ><a href="/categories.html">Categories</a></li>
                            <li ><a href="/tags.html">Tags</a></li>
                            <li ><a href="/archives.html">Archives</a></li>
                            <li><form class="navbar-search" action="/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
<div class="row-fluid">
    <header class="page-header span10 offset2">
    <h1><a href="/posts/2016/07/13/ldasuan-fa-li-jie.html"> <span class="caps">LDA</span>算法理解  </a></h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">

            
            <h3 id="_1">引言</h3>
<p><span class="caps">LDA</span>相关的资料已经非常丰富，比较好的如[1], [2], [3], [4] 和&nbsp;[5]。 </p>
<p>[1]是一篇中文博文，核心的点都提到了，基本参考了[2], [3],&nbsp;[4]的内容。</p>
<p>[2]是一篇科普佳作，结合数学史，把<span class="caps">LDA</span>涉及的很多函数及背后的数据原理通俗地讲了一遍，适合慢慢品读。</p>
<p>由于原始的<span class="caps">LDA</span>训练很慢，[3]主要讲了<span class="caps">LDA</span>的并行化实现，但是文献对<span class="caps">LDA</span>使用Gibbs采样涉及的公式进行了严密的推导，适合了解算法推导的每个环节。</p>
<p>[4]是较早一篇比较详细讲解<span class="caps">LDA</span>的论文，基本上[1], [2], [3]都参考了该文献, 并且GibbsLDA++&nbsp;的实现，也是主要参考该论文。</p>
<p>[5]讲述了<span class="caps">LDA</span>涉及的概率图模型的更多相关知识，但是还是需要有一定概率图理论基础。</p>
<p>总的来说，<span class="caps">LDA</span>算法的推导还是需要一定数理基础，但是Gibbs采样的实现却非常简单，伪代码一看就懂。</p>
<h3 id="_2">模型理解</h3>
<p>简单地再次总结下要点。</p>
<p>概率基础：</p>
<ol>
<li>多项分布</li>
<li>狄利克雷分布：容易发现先验是狄利克雷分布，似然是多项分布，两者相乘得到的后验也是狄利克雷分布</li>
<li>Delta函数，Gama函数： 狄利克雷分布函数中有一个Delta函数，有的地方也说Beta函数，可以表示成Gama函数的形式，而Gama(x+1) = x *&nbsp;Game(x)</li>
</ol>
<p>Gibbs Sampling&nbsp;推导过程：</p>
<ol>
<li>写出主题和词的联合概率公式，利用狄利克雷分布连乘部分积分等于Delta函数的性质，可以将联合分布只用Delta函数表示</li>
<li>写出排除当前词主题的条件概率公式，其实就是1中得到的两个联合概率的比值，全部都是Delta函数，而Delta函数又都可以表示成Gama函数，利用Gama(x+1) = x *&nbsp;Game(x)的性质，得到最终结果，这个就是Gibbs采样公式</li>
<li>采样结束后，得到每篇文档每个词的主题，根据Dirichlet分布的期望，得到doc-topic这个多项分布的参数theta，以及topic-word这个多项分布的参数phi</li>
</ol>
<h3 id="gibbslda">GibbsLDA++代码剖析</h3>
<p>相关编译和运行可以直接参考官方文档，这里只剖析一下代码。</p>
<p>整个代码的实现偏C风格，结构很清晰，注释也很丰富。</p>
<ul>
<li>lda.cpp&nbsp;主函数</li>
<li>model.h&nbsp;定义了类model，核心实现</li>
<li>dataset.h&nbsp;定义了类document和类dataset，存储输入文档数据</li>
<li>constants.h 定义了<span class="caps">BUFF</span>值和模型状态值</li>
<li>strtokenizer.h&nbsp;定义类strtokenizer，分割文本并保存token</li>
<li>utils.h 定义类&nbsp;utils，包含解析参数，生成模型保存名称，排序等函数</li>
</ul>
<p>主要是理解类model的实现，<span class="caps">LDA</span>训练相关参数如下，其中alpha和beta对K维都是一样的：</p>
<div class="highlight"><pre>// --- model parameters and variables ---    
int M; // dataset size (i.e., number of docs)
int V; // vocabulary size
int K; // number of topics
double alpha, beta; // LDA hyperparameters 
int niters; // number of Gibbs sampling iterations
int liter; // the iteration at which the model was saved
int savestep; // saving period
int twords; // print out top words per each topic
int withrawstrs;

double * p; // temp variable for sampling
int ** z; // topic assignments for words, size M x doc.size()
int ** nw; // cwt[i][j]: number of instances of word/term i assigned to topic j, size V x K
int ** nd; // na[i][j]: number of words in document i assigned to topic j, size M x K
int * nwsum; // nwsum[j]: total number of words assigned to topic j, size K
int * ndsum; // nasum[i]: total number of words in document i, size M
double ** theta; // theta: document-topic distributions, size M x K
double ** phi; // phi: topic-word distributions, size K x V
</pre></div>


<p>关键函数如下：</p>
<div class="highlight"><pre>// init for estimation
int init_est();
int init_estc();

// estimate LDA model using Gibbs sampling
void estimate();
int sampling(int m, int n);
void compute_theta();
void compute_phi();
</pre></div>


<p>init_est()从头开始训练，init_estc()继续训练。文档为M，词表为V，主题数为K，初始状态下，每个词对主题k的数目nw[w][k]都为0，每个文档对主题k的数目nd[m][k]也都为0，每个主题的数目nwsum[k]均为0，每篇文档的词数ndsum[m]均为0。然后对于每篇文档的每个词，随机采样一个主题，更新数组nw, nd, nwsum, ndsum。theta数组为M*K，&nbsp;phi数组为K*V。</p>
<div class="highlight"><pre>z = new int*[M];
for (m = 0; m &lt; ptrndata-&gt;M; m++) {
int N = ptrndata-&gt;docs[m]-&gt;length;
z[m] = new int[N];

    // initialize for z
    for (n = 0; n &lt; N; n++) {
        int topic = (int)(((double)random() / RAND_MAX) * K);
        z[m][n] = topic;

        // number of instances of word i assigned to topic j
        nw[ptrndata-&gt;docs[m]-&gt;words[n]][topic] += 1;
        // number of words in document i assigned to topic j
        nd[m][topic] += 1;
        // total number of words assigned to topic j
        nwsum[topic] += 1;
    } 
    // total number of words in document i
    ndsum[m] = N;      
}
</pre></div>


<p>estimate()进行训练，对于每篇文档的每个词，采样一个主题，然后保存模型，并且计算theta和phi：</p>
<div class="highlight"><pre>// for all z_i
for (int m = 0; m &lt; M; m++) {
    for (int n = 0; n &lt; ptrndata-&gt;docs[m]-&gt;length; n++) {
    // (z_i = z[m][n])
    // sample from p(z_i|z_-i, w)
    int topic = sampling(m, n);
    z[m][n] = topic;
    }
}
</pre></div>


<p>sampling(int m, int&nbsp;n)函数使用Gibbs采样公式计算当前词的主题分布，然后随机选择一个主题，用到了累计概率值高于随机值的方式，类似轮盘赌：</p>
<div class="highlight"><pre>int model::sampling(int m, int n) {
    // remove z_i from the count variables
    int topic = z[m][n];
    int w = ptrndata-&gt;docs[m]-&gt;words[n];
    nw[w][topic] -= 1;
    nd[m][topic] -= 1;
    nwsum[topic] -= 1;
    ndsum[m] -= 1;

    double Vbeta = V * beta;
    double Kalpha = K * alpha;    
    // do multinomial sampling via cumulative method
    for (int k = 0; k &lt; K; k++) {
    p[k] = (nw[w][k] + beta) / (nwsum[k] + Vbeta) *
            (nd[m][k] + alpha) / (ndsum[m] + Kalpha);
    }
    // cumulate multinomial parameters
    for (int k = 1; k &lt; K; k++) {
    p[k] += p[k - 1];
    }
    // scaled sample because of unnormalized p[]
    double u = ((double)random() / RAND_MAX) * p[K - 1];

    for (topic = 0; topic &lt; K; topic++) {
    if (p[topic] &gt; u) {
        break;
    }
    }

    // add newly estimated z_i to count variables
    nw[w][topic] += 1;
    nd[m][topic] += 1;
    nwsum[topic] += 1;
    ndsum[m] += 1;

    return topic;
}
</pre></div>


<p>compute_theta()函数调用了推导公式：
</p>
<div class="math">\begin{align*} 
\hat{\theta}_{mk} &amp;= \frac{n_{m}^{(k)} + \alpha_k}{\sum_{k=1}^K (n_{m}^{(k)} + \alpha_k)} 
\end{align*}</div>
<div class="highlight"><pre>for (int m = 0; m &lt; M; m++) {
for (int k = 0; k &lt; K; k++) {
    theta[m][k] = (nd[m][k] + alpha) / (ndsum[m] + K * alpha);
}
}
</pre></div>


<p>compute_phi()函数调用了推导公式：
</p>
<div class="math">\begin{align*} 
\hat{\varphi}_{kw} &amp;= \frac{n_{k}^{(w)} + \beta_t}{\sum_{w=1}^V (n_{k}^{(w)} + \beta_w)} 
\end{align*}</div>
<div class="highlight"><pre>for (int k = 0; k &lt; K; k++) {
for (int w = 0; w &lt; V; w++) {
    phi[k][w] = (nw[w][k] + beta) / (nwsum[k] + V * beta);
}
}
</pre></div>


<p>对新的数据推导主题分布时，重新走了一遍Gibbs采样，相应的计算theta和phi的公式不变，但有略微调整，这里不再赘述。</p>
<h3 id="lda"><span class="caps">LDA</span>开源工具包</h3>
<ul>
<li><a href="http://gibbslda.sourceforge.net/">GibbsLDA++</a></li>
<li><a href="https://code.google.com/p/plda/">plda</a> (<a href="https://github.com/openbigdatagroup/plda">地址2</a>)</li>
</ul>
<h3 id="_3">相关参考</h3>
<ul>
<li>[1] <a href="http://blog.csdn.net/yangliuy/article/details/8302599">概率语言模型及其变形系列(2)-<span class="caps">LDA</span>及Gibbs&nbsp;Sampling</a></li>
<li>[2] <a href="http://www.52nlp.cn/lda-math-%E6%B1%87%E6%80%BB-lda%E6%95%B0%E5%AD%A6%E5%85%AB%E5%8D%A6"><span class="caps">LDA</span>数学八卦</a> (<a href="http://vdisk.weibo.com/s/q0sGh/1360334108">pdf</a>)</li>
<li>[3] Distributed Gibbs Sampling of Latent Topic
Models: The Gritty Details (<a href="https://cxwangyi.files.wordpress.com/2012/01/llt.pdf">pdf</a>)</li>
<li>[4] Parameter estimation for text analysis (<a href="http://www.arbylon.net/publications/text-est.pdf">pdf</a>)</li>
<li>[5] Graphical Representation, Generative Model and Gibbs Sampling (<a href="http://home.in.tum.de/~xiaoh/pub/3G_talk.pdf">pdf</a>)</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript'; 
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            
            
            <hr/>
<section>
    <h2>Keep Reading</h2>
<ul class="related-posts-list">
<li><a href="/posts/2016/10/22/labeled-ldasuan-fa-li-jie.html" title="Labeled-LDA算法理解">Labeled-LDA算法理解</a></li>
</ul>
<hr />
</section>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="previous-article">« <a href="/posts/2016/07/03/qing-hai-xing.html" title="Previous: 青海行">青海行</a></li>
                <li class="next-article"><a href="/posts/2016/07/22/pythondai-li-xia-zai.html" title="Next: python代理下载">python代理下载</a> »</li>
            </ul>
            </nav>
            </aside>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2016-07-13T00:00:00+08:00"> 7月 13, 2016</time>
            <h4>Category</h4>
            <a class="category-link" href="/categories.html#tech-ref">Tech</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="/tags.html#lda-ref">lda
                    <span>1</span>
</a></li>
                <li><a href="/tags.html#zhu-ti-mo-xing-ref">主题模型
                    <span>2</span>
</a></li>
            </ul>
<h4>Stay in Touch</h4>
    <a href="http://www.weibo.com/u/2272983007" title="My weibo Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-weibo sidebar-social-links"></i></a>
    <a href="http://twitter.com/SeptWinds" title="My twitter Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-twitter sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-license"><span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">当你老了</span> by <a xmlns:cc="http://creativecommons.org/ns#" href="http://tangke.me" property="cc:attributionName" rel="cc:attributionURL">JimmyTang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/deed.en_US">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.</li>
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>            <script src="http://code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    
    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>