{"pages":[{"url":"/posts/2016/07/22/pythondai-li-xia-zai.html","text":"当用同一个ip不断地去爬取一个网站的时候，很可能封掉，这时候就需要用到代理来分散请求。 分两种情况 代理设置了dns，调用函数crawl_page2() 代理没有设置dns，调用函数crawl_page()，这种情况稍微复杂点，先要获取url域名对应的ip地址，给http请求包加上dest_ip字段 代码如下 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 #!/usr/bin/env python # encoding:utf-8 import sys import time import urllib2 import socket import struct def get_dest_ip ( domain ): ip_addr = socket . gethostbyname ( domain ) #just for ipv4 #ip_addr = socket.getaddrinfo(domain, 'http')[0][4][0] #[(2, 1, 6, '', ('14.215.177.38', 80)), (2, 2, 17, '', ('14.215.177.38', 80)), (2, 1, 6, '', ('14.215.177.37', 80)), (2, 2, 17, '', ('14.215.177.37', 80))] uint32_binary_str = socket . inet_aton ( str ( ip_addr )) unpack_result = struct . unpack ( \"!I\" , uint32_binary_str ) ip_int = socket . htonl ( unpack_result [ 0 ]) return ip_int def crawl_page ( url , dest_ip , cur_proxy ): ''' ''' content = '' myheaders = { \"User-Agent\" : \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.2; .NET CLR 1.0.3705;)\" , \"Proxy-Connection\" : \"Keep-Alive\" , \"dest_ip\" : dest_ip } p = \"http:// %s \" % cur_proxy h = urllib2 . ProxyHandler ({ \"http\" : p }) o = urllib2 . build_opener ( h , urllib2 . HTTPHandler ) o . addheaders = myheaders . items () try : r = o . open ( url , timeout = 5 ) content = r . read () except urllib2 . HTTPError , e : print \"Error Code:\" , e . code if e . code == 404 : print \"No page: %s \" % url except urllib2 . URLError , e : print \"Error Reason:\" , e . reason except Exception as e : print \"Error\" , str ( e ) if len ( content ) > 10 : print \"Good \\t %s \" % p else : print \"Bad \\t %s \" % p return content def crawl_page2 ( url , cur_proxy = '' ): ''' get ''' # print \"-->crawl comment: %s\" % url # print \"-->cur_proxy: %s\" % cur_proxy myheaders = { \"User-Agent\" : \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.2; .NET CLR 1.0.3705;)\" } content = '' try : if cur_proxy : proxy_handler = urllib2 . ProxyHandler ({ 'http' : cur_proxy }) opener = urllib2 . build_opener ( proxy_handler ) f = opener . open ( url , timeout = 5 ) content = f . read () else : req = urllib2 . Request ( url , headers = myheaders ) f = urllib2 . urlopen ( req , timeout = 5 ) content = f . read () except urllib2 . HTTPError , e : print \"Error Code:\" , e . code if e . code == 404 : print \"No page: %s \" % url except urllib2 . URLError , e : print \"Error Reason:\" , e . reason except Exception as e : print \"Error\" , str ( e ) time . sleep ( 1 ) return content if __name__ == '__main__' : # open proxys proxy_list = open ( sys . argv [ 1 ]) . readlines () url = \"http://www.baidu.com\" host = \"www.baidu.com\" dest_ip = str ( get_dest_ip ( host )) for p in proxy_list : p = p . strip () n1 = len ( crawl_page ( url , dest_ip , p )) print \"crawl_page len:\" , n1 n2 = len ( crawl_page2 ( url , p )) print \"crawl_page2 len:\" , n2 测试输出 输入文件为两个代理ip，一个配置了dns，一个没有配置dns Good http://10.183.27.147:32810 crawl_page len: 10811 crawl_page2 len: 10811 Good http://10.184.16.44:32810 crawl_page len: 10811 Error Reason: timed out crawl_page2 len: 0 其他说明 获取dest_ip是访问的dns服务，并不是访问的原网站，而dns是带本地cache，所以频繁访问应该是没有问题的。 函数get_dest_ip()中调用了socket模块的相关函数: socket.gethostbyname() 获取域名对应ip，只支持ipv4，若需要支持ipv6，可使用函数socket.getaddrinfo() socket.inet_aton() 转换ip地址（192.168.1.10）为32位打包二进制字符串，只支持ipv4，若需要支持ipv6，可使用函数socket.inet_pton() socket.htonl() 将32位整数从主机字节序转换成网络字节序 参考 http://www.programgo.com/article/11342723643/ http://www.cnblogs.com/gala/archive/2011/09/22/2184801.html","tags":"Tech","title":"python代理下载"},{"url":"/posts/2015/07/04/dlibshi-yong-ru-men.html","text":"dlib是一个比较知名的C++机器学习包，集成常用的算法，文档清晰，实例丰富，初步使用了一下，上手较快，编译也比较简单。 安装 首先从官网下载工具包，然后按照官网教程进行安装就可以了。 聚类实验 尝试使用dlib提供的kmeans算法进行了一次聚类实验。 …","tags":"Tech","title":"dlib使用入门"},{"url":"/posts/2015/05/10/sha-men-san-ri-you.html","text":"从5月8日到5月10日，跟着豪华旅行团，在厦门开心地玩了3天。 第一天 下午坐动车去，落脚酒店,安顿下来后，去夜市逛了逛。 第二天 这一天包括了主要的行程：上午先在海边骑了自行车，然后去了炮台，下午则去了这次旅行的主要目标——鼓浪屿岛。 第三天 去了南普陀和厦门大学。因为赶车，比较匆忙地看了看。","tags":"Life","title":"厦门三日游"},{"url":"/posts/2015/01/24/jian-zhan-bei-wang.html","text":"一直想着搭建一个自己的网站，这次终于付诸行动了。参考了网上的许多资料，所以在这里就简单记录下自己的建站过程。 购买域名 从godaddy购买域名。也可用选择从万网和新网购买域名，这些网站还提供一条龙的服务，如提供主机，帮助备案等。 选择托管网站的主机 购买虚拟主机或者 VPS ，也可选择存储在github上，我选择的后者（以后有时间再买个主机试试）。如果要将网页放在github上，需要申请一个github账号，并创建一个与帐号同名的项目。具体参考 GitHub Pages 。 生成网页 Github推荐使用Jekyll构建自己的github pages。由于偏好python，我选择的pelican（鹈鹕）。Pelican是一个python包，使用pelican生成博客框架的过程如下： sudo apt-get install python-pip pip install pelican pip install markdown mkdir myblog cd myblog pelican-quickstart 在myblog/content目录下编写markdown文件：test.md,然后在myblog目录下执行如下命令： make html make serve 在myblog/output目录下会生成页面，同时在浏览器输入 localhost:8000 可以查看结果。 Markdown语法可参考 Markdown Basics ，Ubuntu-14.04下貌似默认安装了编辑器retext。 此外，需要加入如下几个功能： 选择博客主题。下载pelican主题项目进行配置。 git clone --recursive https://github.com/getpelican/pelican-themes.git cd pelican-themes pelican-themes -i ./pelican-themes/elegent 增加sitemap。下载pelican插件项目进行配置。 git clone https://github.com/getpelican/pelican-plugins cd pelican-plguins 增加评论。 disqus 提供了评论功能，注册账号即可获取一个shortname，将shortname加入pelicanconf.py,生成的页面中就会加入评论功能。 增加站长统计。可选择google或者百度的站长工具。如果选择gogole，在 Google Analytics 创建帐号，将追踪 ID 加入pelicanconf.py，生成页面中就会加入追踪功能。在 Google Webmasters 可查看追踪结果。 最后，配置文件myblog/pelicanconf.py基本如下： AUTHOR = u'lostfish' SITENAME = u\"lostfish\" SITEURL = 'http://tangke.me' THEME = 'elegent' DISQUS_SITENAME = 'disqus提供的shortname' GOOGLE_ANALYTICS = 'google提供的追踪id' PATH = 'content' #网页内容对应的markdown文件路径 ... # 设置生成页面存储路径为pots/年/月/日/{slug}.html (slug即文章标题拼音) USE_FOLDER_AS_CATEGORY = False ARTICLE_URL = 'posts/{date:%Y}/{date:%m}/{date:%d}/{slug}.html' ARTICLE_SAVE_AS = 'posts/{date:%Y}/{date:%m}/{date:%d}/{slug}.html' PAGE_URL = 'pages/{slug}.html' PAGE_SAVE_AS = 'pages/{slug}.html' YEAR_ARCHIVE_SAVE_AS = 'posts/{date:%Y}/index.html' MONTH_ARCHIVE_SAVE_AS = 'posts/{date:%Y}/{date:%m}/index.html' #站点地图插件配置 PLUGIN_PATHS = [\"pelican-plugins\"] PLUGINS = ['sitemap', 'extract_toc', 'tipue_search', 'liquid_tags.img', 'neighbors', 'latex', 'related_posts', 'share_post'] SITEMAP = { 'format': 'xml', 'priorities': { 'articles': 0.7, 'indexes': 0.5, 'pages': 0.3, }, 'changefreqs': { 'articles': 'monthly', 'indexes': 'daily', 'pages': 'monthly' } } 绑定域名 将自己购买的域名与github page绑定在一起需要3步： 在github pages项目下增加文件 CNAME ，在该文件中加入购买的域名。 在godaddy中将购买域名对应的域名解析服务器绑定到dnspod的服务器，可参考: Godaddy注册商域名修改 DNS 地址 。 注册dnspod账户，将自己购买的域名与github的服务器绑定,建立两条A类记指向192.30.252.153和192.30.252.154（这个 IP 可能会变化，参考github上 教程 ）。另外，这个配置最晚可能要等72小时之后才生效。 有用参考 Pelican官方文档 Pelican主题列表 主题elegent配置说明","tags":"Tech","title":"建站备忘"}]}