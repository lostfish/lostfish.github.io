{"pages":[{"url":"/posts/2016/10/22/labeled-ldasuan-fa-li-jie.html","text":"引言 Labeled LDA (L- LDA )是一个有监督的机器学习模型，主要应用是多类标分类，即给一篇文档打上多个类标。与 LDA 最大的不同之处为：L- LDA 的主题限定为文档的类标集合，也就是说生成过程中的主题就是这些类标，不用人工指定主题数。 模型理解 L- LDA 模型与原始 LDA 模型基本相同，变化很小，下面的理解全部参考原始论文[1]。 L- LDA 概率图模型如下： 去掉左下角Phi和Lamda所指向两条边就是原始的 LDA 主题模型。 其中： N(d) 是文档d的词数目 K是主题数目，也是类标数目，在L- LDA 中，主题和类标是等价的 Alpha是主题共轭先验分布的参数，K维，Dirichlet分布（预先指定） Eta是词共轭先验分布的参数，K维，Dirichlet分布（预先指定） Eta(k)是主题k的词分布（多项分布） Theta(d)是文档d的主题分布（多项分布） Phi是主题先验分布，或者说类标先验分布，K维（预先统计） Lambda(d)为K维指示向量，每一维度取值0或者1，表示文档d是否属于这个主题 生成过程如下： 说明： 步骤1和2生成每个主题下的词分布，和原始 LDA 一样 步骤4-5统计文档d的K维指示向量，1表示文档d属于这个主题，0表示不属于，进而可以得到文档d的类标投射矩阵L(d),通过伯努利实验生成。假设有4个主题，文档d属于主题2和主题3， 那么L(d)表示为2*4的矩阵（第一行仅第2列为1， 第二行仅第3列为1）： 0 1 0 0 0 0 1 0 步骤6根据类标投射矩阵L(d)得到文档d的超参数向量alpha(d)，原始alpha是K维，投影后变为M(d)维，其实就是文档d的类标数。还是使用上面的例子，文档的d包含主题2和3，那么这里M(d)就等于2，Alpha(d) = L(d)*Alpha = {alpha2, alpha3} 步骤7根据alpha(d)生成M(d)个主题的分布theta(d)，与原始 LDA 不同，此处将主题限定在文档d对应的M(d)个主题中 步骤8-10 同原始 LDA 一样，先生成主题，再根据主题生成词 注 ：步骤4-6可能是论文为了形式化，写的有点复杂，实际实现的时候没有这个伯努利实验的过程，直接从训练数据得到文档的类标，训练的时候主题限定在这个类标集合内进行Gibbs采样。 算法实现 搜索了一下，有如下的开源实现： http://www-nlp.stanford.edu/software/tmt/tmt-0.4/ (scala) https://github.com/myleott/JGibbLabeledLD (java） https://github.com/shuyo/iir/blob/master/lda/llda.py （python） 但是没有看到C++的，其实可以基于 GibbsLDA++ 来改，只需要很小的改动就可以实现，这里说下大体思路，等有时间再实现： 文档结构体中保留每篇文档的类标 Gibbs采样的时候，只在当前文档对应的类标集合里面采样，如果文档都是单类标，那么就不用采样。 预测函数不用变化，在新文档上得到所有主题的概率。如果是单类标分类，那么输出概率最大的主题；如果是多类标分类，输出大于一定阈值的所有主题。 相关参考 [1] Labeled LDA : A supervised topic model for credit attribution in multi-labeled corpora ( pdf )","tags":"Tech","title":"Labeled- LDA 算法理解"},{"url":"/posts/2016/09/17/textranksuan-fa-shi-xian.html","text":"算法原理 TextRank算法主要用于文档的关键词抽取或者摘要抽取（本质是关键句抽取），原理就是PageRank的那一套思想，只不过构建图模型的时候稍有区别。 抽取关键词的时候，图的节点为词，边为滑动窗口中共现的词对，边的权重为两个词在滑动窗口中的共现次数。不是所有的词都考虑，一般只选取特定词性的词，如名词，形容词和动词。同时，滑动窗口也有一个长度，比如连续5个词。 抽取关键句的时候，图的节点为句子，边为文档中所有句子的两两组合，边的权重为两个句子的相似度，可分词后计算Jaccard相似度，或者更复杂点的提取语义向量计算余弦相似度，前者是原始论文[1]中的做法: \\begin{align*} Similarity(S_i,S_j)=\\frac {|\\{w_k|w_k\\in S_i \\& w_k\\in S_j\\}|}{log(|{S_i}|)+log(|S_j|)} \\end{align*} 有了图模型，就可以迭代计算每个节点的权重，然后对节点进行排序。[1]中给出的公式为： \\begin{align*} WS(V_i) = (1 - d) + d * \\sum_{V_j\\in In(V_i)} \\frac{w_{ji}}{\\sum_{V_k\\in Out(V_j)} w_{jk}}WS(V_j) \\end{align*} 原始PageRank公式为： \\begin{align*} S(V_i) = (1 - d) + d * \\sum_{j\\in In(V_i)} \\frac{1}{|Out(V_j)|}S(V_j) \\end{align*} 可以看出主要区别为：在PageRank中边的权重都是1，而在TextRank中边的权重是变化的。 算法实现 由于算法原理比较简单，实现起来也比较容易，用C++实现了一个版本，github地址为： https://github.com/lostfish/textrank 实现的关键是构建图。为了简化，同时原始论文中的实验也说明有向图和无向图得到的结果相差不大，所以这里构建的是无向加权图。 为节省空间，将词都映射为size_t类型的词 ID 。同时，用pair 表示边，用map , double> 存储边的权重。为计算方便，需要保留一个节点所有出度边的权重和，用map 来表示。 核心rank的过程见text_rank.cpp中的函数CalcWordScore()，终止条件为两个：一个是两次迭代词权重的变化值小于预先设定的阈值，如0.0001，另一个是达到最大迭代次数，如100次。满足任一条件都终止继续迭代，实验中发现一般迭代几十次就会终止。 实验效果 仅仅和tf抽取的结果做了对比，没和tf-idf作对比是因为idf先验的获取依赖具体应用。实验数据集为新闻长文档，发现text_rank的效果比按照tf排序的结果稍好。 具体实验数据待整理。 貌似在具体的应用上，现在还没有特别好的关键词抽取算法，就是那种比经典的tf-idf算法效果好很多的。在一般抽取关键词的场景下，根据具体应用的数据，整理一份idf先验词表，然后用tf-idf算法其实就可以了。当然候选词要精心筛选，至少有根据词性过滤。 相关参考 [1] TextRank: Bringing Order into Texts if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); var location_protocol = (false) ? 'https' : document.location.protocol; if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:'; mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Tech","title":"TextRank算法实现"},{"url":"/posts/2016/08/27/eclatsuan-fa-shi-xian.html","text":"基本原理 Eclat是挖掘频繁项集的一种算法，相比较而言远，它远没有另外两种算法Apriori和 FP -Growth那么出名，一个直接的证明就是以前读书的时候课本都没有介绍。 不过这个算法也有它自身的特点，基本原理和Apriori算法很相似，也是通过频繁K项集连接生成频繁K+1项集，但是比Apriori速度快很多，因为引入了倒排机制，保留了每个频繁项集对应的所有记录。这样，在连接生成频繁K+1项集的过程中，两个频繁K项集对应记录集合求交，就得到了这个K+1项集对应的记录集合，这个集合的size就是K+1项集的出现频数，通过与支持度比较就可以判断新产生的K+1项集是不是频繁的。从这个过程可以看出，Eclat算法只需要扫描一次记录库，而Apriori算法每次判断候选K+1项集是不是频繁的时候都要扫描一次记录库，所以Eclat比Apriori快也就不奇怪了。不难看出，Eclat算法其实是用空间换取了时间，后续实验也证明了这个算法是比较耗费空间的。 举个例子（参考[1])： 有4条记录 tid item 1 A,B 2 B,C 3 A,C 4 A,B,C 设定最小支持度为2，第一边扫描记录库后，得到频繁1项集，其中每个item对应的记录集合也保留了，如下： item tids freq A 1,3,4 3 B 1,2,4 3 C 2,3,4 3 由频繁1项集连接生成候选2项集，都是频繁的： item tids freq A,B 1,4 2 A,C 3,4 2 B,C 2,4 2 由频繁2项集连接生成候选3项集，只有一个候选并且不是频繁的，算法结束： item tids freq A,B,C 4 1 注意 ：在最后一步中，只有A,B和A,C前缀相同，可以连接 实现思路 尝试用C++进行了基本的实现。这里总结几个要点： 所有item都转换为整数表示，项集用存储item的有序vector表示，这样连接生成K+1项集的时候方便快速比较前缀 每个频繁项集和记录集合的映射关系用map，记录集合用set 连接过程关键是比较前缀，比如有两个K项集，那么比较前k-1项是否相同，相同就把其中一个K项集最后一项追加到另一个最后，构成K+1项集，追加后要保证K+1项集是有序的。 从算法过程中还可以看出：频繁K项集的计算，需要把前面频繁1,2，.., k-1项集都计算后才能进行。此外，频繁1项集和频繁2项集的计算需要特殊处理，不用比较前缀。 具体实现参考代码： https://github.com/lostfish/eclat 实验结果 测试数据采用和[2]相同的mushroom.dat，该数据集下载地址为[3]，下载后需要去掉每行最后的空格。 机器配置为：Intel(R) Xeon(R) CPU E5606 @ 2.13GHz (8核，32G内存) 测试结果如下： 最小支持度 频繁项集数 运行时间 812(0.1) 574513 29.219 1218(0.15) 98575 4.593 1624(0.2) 53663 2.632 2031(0.25) 5545 0.603 当支持度为812时，挖掘的各频繁项集数目如下： valid_num : 8124 8124 uniq id num : 119 frequent 1 - itemset size : 56 frequent 2 - itemset size : 763 frequent 3 - itemset size : 4599 frequent 4 - itemset size : 16171 frequent 5 - itemset size : 38829 frequent 6 - itemset size : 69854 frequent 7 - itemset size : 98852 frequent 8 - itemset size : 111787 frequent 9 - itemset size : 100660 frequent 10 - itemset size : 71342 frequent 11 - itemset size : 39171 frequent 12 - itemset size : 16292 frequent 13 - itemset size : 4956 frequent 14 - itemset size : 1039 frequent 15 - itemset size : 134 frequent 16 - itemset size : 8 同时还观察了下空间消耗，占用内存达到了将近9G，比较惊人。当然这是因为在计算的过程中，保留了所有频繁集，如果每次计算完就直接输出，会节省很多空间。另外，这个数据集合本身频繁模式就比较多，平时应用中一般也不需要计算所有频繁项集，一般到频繁3项集或4项集就够了。 相关参考 [1] https://zh.wikipedia.org/zh/%E5%85%B3%E8%81%94%E5% BC %8F%E8%A7%84%E5%88%99 [2] http://blog.csdn.net/yangliuy/article/details/7494983 [3] http://fimi.ua.ac.be/data/mushroom.dat","tags":"Tech","title":"Eclat算法实现"},{"url":"/posts/2016/07/22/pythondai-li-xia-zai.html","text":"当用同一个ip不断地去爬取一个网站的时候，很可能封掉，这时候就需要用到代理来分散请求。 分两种情况 代理设置了dns，调用函数crawl_page2() 代理没有设置dns，调用函数crawl_page()，这种情况稍微复杂点，先要获取url域名对应的ip地址，给http请求包加上dest_ip字段 代码如下 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 #!/usr/bin/env python # encoding:utf-8 import sys import time import urllib2 import socket import struct def get_dest_ip ( domain ): ip_addr = socket . gethostbyname ( domain ) #just for ipv4 #ip_addr = socket.getaddrinfo(domain, 'http')[0][4][0] #[(2, 1, 6, '', ('14.215.177.38', 80)), (2, 2, 17, '', ('14.215.177.38', 80)), (2, 1, 6, '', ('14.215.177.37', 80)), (2, 2, 17, '', ('14.215.177.37', 80))] uint32_binary_str = socket . inet_aton ( str ( ip_addr )) unpack_result = struct . unpack ( \"!I\" , uint32_binary_str ) ip_int = socket . htonl ( unpack_result [ 0 ]) return ip_int def crawl_page ( url , dest_ip , cur_proxy ): ''' ''' content = '' myheaders = { \"User-Agent\" : \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.2; .NET CLR 1.0.3705;)\" , \"Proxy-Connection\" : \"Keep-Alive\" , \"dest_ip\" : dest_ip } p = \"http:// %s \" % cur_proxy h = urllib2 . ProxyHandler ({ \"http\" : p }) o = urllib2 . build_opener ( h , urllib2 . HTTPHandler ) o . addheaders = myheaders . items () try : r = o . open ( url , timeout = 5 ) content = r . read () except urllib2 . HTTPError , e : print \"Error Code:\" , e . code if e . code == 404 : print \"No page: %s \" % url except urllib2 . URLError , e : print \"Error Reason:\" , e . reason except Exception as e : print \"Error\" , str ( e ) if len ( content ) > 10 : print \"Good \\t %s \" % p else : print \"Bad \\t %s \" % p return content def crawl_page2 ( url , cur_proxy = '' ): ''' get ''' # print \"-->crawl comment: %s\" % url # print \"-->cur_proxy: %s\" % cur_proxy myheaders = { \"User-Agent\" : \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.2; .NET CLR 1.0.3705;)\" } content = '' try : if cur_proxy : proxy_handler = urllib2 . ProxyHandler ({ 'http' : cur_proxy }) opener = urllib2 . build_opener ( proxy_handler ) f = opener . open ( url , timeout = 5 ) content = f . read () else : req = urllib2 . Request ( url , headers = myheaders ) f = urllib2 . urlopen ( req , timeout = 5 ) content = f . read () except urllib2 . HTTPError , e : print \"Error Code:\" , e . code if e . code == 404 : print \"No page: %s \" % url except urllib2 . URLError , e : print \"Error Reason:\" , e . reason except Exception as e : print \"Error\" , str ( e ) time . sleep ( 1 ) return content if __name__ == '__main__' : # open proxys proxy_list = open ( sys . argv [ 1 ]) . readlines () url = \"http://www.baidu.com\" host = \"www.baidu.com\" dest_ip = str ( get_dest_ip ( host )) for p in proxy_list : p = p . strip () n1 = len ( crawl_page ( url , dest_ip , p )) print \"crawl_page len:\" , n1 n2 = len ( crawl_page2 ( url , p )) print \"crawl_page2 len:\" , n2 测试输出 输入文件为两个代理ip，一个配置了dns，一个没有配置dns Good http://10.183.27.147:32810 crawl_page len: 10811 crawl_page2 len: 10811 Good http://10.184.16.44:32810 crawl_page len: 10811 Error Reason: timed out crawl_page2 len: 0 其他说明 获取dest_ip是访问的dns服务，并不是访问的原网站，而dns是带本地cache，所以频繁访问应该是没有问题的。 函数get_dest_ip()中调用了socket模块的相关函数: socket.gethostbyname() 获取域名对应ip，只支持ipv4，若需要支持ipv6，可使用函数socket.getaddrinfo() socket.inet_aton() 转换ip地址（192.168.1.10）为32位打包二进制字符串，只支持ipv4，若需要支持ipv6，可使用函数socket.inet_pton() socket.htonl() 将32位整数从主机字节序转换成网络字节序 参考 http://www.programgo.com/article/11342723643/ http://www.cnblogs.com/gala/archive/2011/09/22/2184801.html","tags":"Tech","title":"python代理下载"},{"url":"/posts/2016/07/13/ldasuan-fa-li-jie.html","text":"引言 LDA 相关的资料已经非常丰富，比较好的如[1], [2], [3], [4] 和 [5]。 [1]是一篇中文博文，核心的点都提到了，基本参考了[2], [3], [4]的内容。 [2]是一篇科普佳作，结合数学史，把 LDA 涉及的很多函数及背后的数据原理通俗地讲了一遍，适合慢慢品读。 由于原始的 LDA 训练很慢，[3]主要讲了 LDA 的并行化实现，但是文献对 LDA 使用Gibbs采样涉及的公式进行了严密的推导，适合了解算法推导的每个环节。 [4]是较早一篇比较详细讲解 LDA 的论文，基本上[1], [2], [3]都参考了该文献, 并且GibbsLDA++ 的实现，也是主要参考该论文。 [5]讲述了 LDA 涉及的概率图模型的更多相关知识，但是还是需要有一定概率图理论基础。 总的来说， LDA 算法的推导还是需要一定数理基础，但是Gibbs采样的实现却非常简单，伪代码一看就懂。 模型理解 简单地再次总结下要点。 概率基础： 多项分布 \\begin{align*} \\displaystyle Mult(\\overrightarrow{n} |\\overrightarrow{p},N) = \\binom{N}{\\overrightarrow{n}}\\prod_{k=1}&#94;K p_k&#94;{n_k} \\end{align*} 狄利克雷分布：容易发现先验是狄利克雷分布，似然是多项分布，两者相乘得到的后验也是狄利克雷分布 \\begin{align*} Dir(\\overrightarrow{p}|\\overrightarrow{\\alpha}) &= \\frac{1}{\\Delta(\\overrightarrow{\\alpha})} \\prod_{k=1}&#94;K p_k&#94;{\\alpha_k-1} \\end{align*} \\begin{align*} \\int_{\\overrightarrow{p}}&#94;{} \\prod_{k=1}&#94;K p_k&#94;{\\alpha_k-1} = \\Delta(\\overrightarrow{\\alpha}) \\end{align*} - Delta函数/Gama函数： 狄利克雷分布函数中有一个Delta函数，有的地方也说Beta函数，可以表示成Gama函数的形式，而Gama(x+1) = x * Game(x) \\begin{align*} \\Delta(\\overrightarrow{\\alpha}) = \\frac{\\prod_{k=1}&#94;K\\Gamma(\\alpha_k)} {\\Gamma(\\sum_{k=1}&#94;K\\alpha_k)} \\end{align*} Gibbs Sampling 推导过程： 1) 写出主题和词的联合概率公式，利用狄利克雷分布连乘部分积分等于Delta函数的性质，可以将联合分布只用Delta函数表示 \\begin{align} p(\\overrightarrow{\\mathbf{w}},\\overrightarrow{\\mathbf{z}} |\\overrightarrow{\\alpha}, \\overrightarrow{\\beta}) &= p(\\overrightarrow{\\mathbf{w}} |\\overrightarrow{\\mathbf{z}}, \\overrightarrow{\\beta}) p(\\overrightarrow{\\mathbf{z}} |\\overrightarrow{\\alpha}) \\\\ &= \\prod_{k=1}&#94;K p(\\overrightarrow{w}_{(k)} | \\overrightarrow{z}_{(k)}, \\overrightarrow{\\beta}) \\prod_{m=1}&#94;M p(\\overrightarrow{z}_m |\\overrightarrow{\\alpha}) \\\\ &= \\prod_{k=1}&#94;K \\frac{\\Delta(\\overrightarrow{n}_k+\\overrightarrow{\\beta})}{\\Delta(\\overrightarrow{\\beta})} \\end{align} \\begin{align} \\prod_{m=1}&#94;M \\frac{\\Delta(\\overrightarrow{n}_m+\\overrightarrow{\\alpha})}{\\Delta(\\overrightarrow{\\alpha})} \\end{align} 2) 写出排除当前词主题的条件概率公式，其实就是1中得到的两个联合概率的比值，全部都是Delta函数，而Delta函数又都可以表示成Gama函数，利用Gama(x+1) = x * Game(x)的性质，得到最终结果，这个就是Gibbs采样公式 \\begin{align*} p(z_i = k|\\overrightarrow{\\mathbf{z}}_{\\neg i}, \\overrightarrow{\\mathbf{w}}) \\propto \\frac{n_{m,\\neg i}&#94;{(k)} + \\alpha_k}{\\sum_{k=1}&#94;K (n_{m,\\neg i}&#94;{(k)} + \\alpha_k)} \\cdot \\frac{n_{k,\\neg i}&#94;{(t)} + \\beta_t}{\\sum_{t=1}&#94;V (n_{k,\\neg i}&#94;{(t)} + \\beta_t)} \\end{align*} 3) 采样结束后，得到每篇文档每个词的主题，根据Dirichlet分布的期望，得到doc-topic这个多项分布的参数theta，以及topic-word这个多项分布的参数phi GibbsLDA++代码剖析 相关编译和运行可以直接参考官方文档，这里只剖析一下代码。 整个代码的实现偏C风格，结构很清晰，注释也很丰富。 lda.cpp 主函数 model.h 定义了类model，核心实现 dataset.h 定义了类document和类dataset，存储输入文档数据 constants.h 定义了 BUFF 值和模型状态值 strtokenizer.h 定义类strtokenizer，分割文本并保存token utils.h 定义类 utils，包含解析参数，生成模型保存名称，排序等函数 主要是理解类model的实现， LDA 训练相关参数如下，其中alpha和beta对K维都是一样的： // --- model parameters and variables --- int M; // dataset size (i.e., number of docs) int V; // vocabulary size int K; // number of topics double alpha, beta; // LDA hyperparameters int niters; // number of Gibbs sampling iterations int liter; // the iteration at which the model was saved int savestep; // saving period int twords; // print out top words per each topic int withrawstrs; double * p; // temp variable for sampling int ** z; // topic assignments for words, size M x doc.size() int ** nw; // cwt[i][j]: number of instances of word/term i assigned to topic j, size V x K int ** nd; // na[i][j]: number of words in document i assigned to topic j, size M x K int * nwsum; // nwsum[j]: total number of words assigned to topic j, size K int * ndsum; // nasum[i]: total number of words in document i, size M double ** theta; // theta: document-topic distributions, size M x K double ** phi; // phi: topic-word distributions, size K x V 关键函数如下： // init for estimation int init_est(); int init_estc(); // estimate LDA model using Gibbs sampling void estimate(); int sampling(int m, int n); void compute_theta(); void compute_phi(); init_est()从头开始训练，init_estc()继续训练。文档为M，词表为V，主题数为K，初始状态下，每个词对主题k的数目nw[w][k]都为0，每个文档对主题k的数目nd[m][k]也都为0，每个主题的数目nwsum[k]均为0，每篇文档的词数ndsum[m]均为0。然后对于每篇文档的每个词，随机采样一个主题，更新数组nw, nd, nwsum, ndsum。theta数组为M*K， phi数组为K*V。 z = new int*[M]; for (m = 0; m < ptrndata->M; m++) { int N = ptrndata->docs[m]->length; z[m] = new int[N]; // initialize for z for (n = 0; n < N; n++) { int topic = (int)(((double)random() / RAND_MAX) * K); z[m][n] = topic; // number of instances of word i assigned to topic j nw[ptrndata->docs[m]->words[n]][topic] += 1; // number of words in document i assigned to topic j nd[m][topic] += 1; // total number of words assigned to topic j nwsum[topic] += 1; } // total number of words in document i ndsum[m] = N; } estimate()进行训练，对于每篇文档的每个词，采样一个主题，然后保存模型，并且计算theta和phi： // for all z_i for (int m = 0; m < M; m++) { for (int n = 0; n < ptrndata->docs[m]->length; n++) { // (z_i = z[m][n]) // sample from p(z_i|z_-i, w) int topic = sampling(m, n); z[m][n] = topic; } } sampling(int m, int n)函数使用Gibbs采样公式计算当前词的主题分布，然后随机选择一个主题，用到了累计概率值高于随机值的方式，类似轮盘赌： int model::sampling(int m, int n) { // remove z_i from the count variables int topic = z[m][n]; int w = ptrndata->docs[m]->words[n]; nw[w][topic] -= 1; nd[m][topic] -= 1; nwsum[topic] -= 1; ndsum[m] -= 1; double Vbeta = V * beta; double Kalpha = K * alpha; // do multinomial sampling via cumulative method for (int k = 0; k < K; k++) { p[k] = (nw[w][k] + beta) / (nwsum[k] + Vbeta) * (nd[m][k] + alpha) / (ndsum[m] + Kalpha); } // cumulate multinomial parameters for (int k = 1; k < K; k++) { p[k] += p[k - 1]; } // scaled sample because of unnormalized p[] double u = ((double)random() / RAND_MAX) * p[K - 1]; for (topic = 0; topic < K; topic++) { if (p[topic] > u) { break; } } // add newly estimated z_i to count variables nw[w][topic] += 1; nd[m][topic] += 1; nwsum[topic] += 1; ndsum[m] += 1; return topic; } compute_theta()函数调用了推导公式： \\begin{align*} \\hat{\\theta}_{mk} &= \\frac{n_{m}&#94;{(k)} + \\alpha_k}{\\sum_{k=1}&#94;K (n_{m}&#94;{(k)} + \\alpha_k)} \\end{align*} for (int m = 0; m < M; m++) { for (int k = 0; k < K; k++) { theta[m][k] = (nd[m][k] + alpha) / (ndsum[m] + K * alpha); } } compute_phi()函数调用了推导公式： \\begin{align*} \\hat{\\varphi}_{kw} &= \\frac{n_{k}&#94;{(w)} + \\beta_t}{\\sum_{w=1}&#94;V (n_{k}&#94;{(w)} + \\beta_w)} \\end{align*} for (int k = 0; k < K; k++) { for (int w = 0; w < V; w++) { phi[k][w] = (nw[w][k] + beta) / (nwsum[k] + V * beta); } } 对新的数据推导主题分布时，重新走了一遍Gibbs采样，相应的计算theta和phi的公式不变，但有略微调整，这里不再赘述。 LDA 开源工具包 GibbsLDA++ plda ( 地址2 ) 相关参考 [1] 概率语言模型及其变形系列(2)- LDA 及Gibbs Sampling [2] LDA 数学八卦 ( pdf ) [3] Distributed Gibbs Sampling of Latent Topic Models: The Gritty Details ( pdf ) [4] Parameter estimation for text analysis ( pdf ) [5] Graphical Representation, Generative Model and Gibbs Sampling ( pdf ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); var location_protocol = (false) ? 'https' : document.location.protocol; if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:'; mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Tech","title":"LDA 算法理解"},{"url":"/posts/2016/07/03/qing-hai-xing.html","text":"六月是个好月份，适合旅行。 从22号到26号共五天的时间里，跟团去大青海体验了祖国西部的大好河山。 启程 傍晚时分，天气正好，带一丝燥热，跟随大队伍从深圳宝启程啦。 有点漫长的旅程，飞着飞着天就黑了。 到西宁的时候，已经是午夜。一下飞机就感受到了大西部的寒冷。后面更是感觉到了干燥和缺氧。 第一站 青海湖，美，辽阔，Relaxing！ 第二站 茶卡盐湖，白，天空之境，特别的美。 第三站 沙岛，还是青海湖，但是体验沙漠游玩项目：骑骆驼，骑摩托，滑沙。 最后一站 彩虹部落","tags":"Life","title":"青海行"},{"url":"/posts/2015/07/04/sklearnshi-yong-bi-ji.html","text":"sklearn全称为scikit-learn，是目前很流行的一个基于python的机器学习包，基本覆盖了常见的机器学习算法，如分类、回归、聚类、特征选择、模型选择以及数据预处理等任务对应的算法。文档和示例非常丰富，可视化展示也很方便，所以使用者众多，尤其是在 Kaggle 数据分析竞赛中被参赛者广泛使用。 关于该工具包的使用介绍网上已经非常多，所以这里只是整理和记录自己使用的一些心得。 初始入门 最好入门方法就是参考官网的 Tutorials sklearn集成了一些常用的数据集，以方便测试相关算法，封装为datasets模块，如导入分类数据集合iris和digits，导入回归数据集diabetes from sklearn import datasets iris = datasets . load_iris () digits = datasets . load_digits () diabetes = datasets . load_diabetes () 具体如何在这些数据集合应用相关算法，官网Tutorials有详细的介绍，下面重点讲下文本分类。 文本分类 1. 加载数据 数据集选取的是20newsgroups，该数据集包含20个新闻组约2万篇文档，加载方式也是通过sklearn.datasets模块，但是稍有不同，如下 from sklearn.datasets import fetch_20newsgroups twenty_train = fetch_20newsgroups ( subset = 'train' , shuffle = True , random_state = 42 ) 从上面可以看出，加载数据集实际上是调用函数fetch_20newsgroups() 在python命令行输入help(fetch_20newsgroups），可以查看相应参数说明，其中 subset: 指定加载训练集或测试集，或者两者，取值对应'train', ‘test', ‘all' data_home: 指定20newsgroups数据集所在路径，默认为 ‘~/scikit_learn_data'，Windows下对应为C:\\Users\\xxx\\scikit_learn_data categories: 指定要加载的类别list，默认为None，表示所有类别 shuffle： 是否要混洗数据 random_state： 混洗随机数的种子值 download_if_missing： 指定data_home下不存在数据集时是否下载，默认为True，表示下载 remove： 指定预处理文本的过滤策略，取值为元组 (‘headers', ‘footers', ‘quotes') 刚开始测试这个数据集的时候，老是等半天没有结果。于是看了下对应的源代码./site-packages/sklearn/datasets/twenty_newsgroups.py，发现第一次加载的时候会下载数据集到~/scikit_learn_data/20news_home，下载完成后会解压然后压缩生成cache文件，即~/scikit_learn_data/20news-bydate.pkz，以后每次加载就读取该文件了。 所以首次加载的时候需要有点耐心，大概等待7分钟左右吧（国内环境）。 当然也可以自己下载放到目录~/scikit_learn_data/20news_home，下载地址为： http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz （文件大小为13.7M） 2. 特征选择 最常见的就是词袋模型，每个词就是一个特征，特征权重为词频，通过构建 CountVectorizer 来向量化每篇文档： from sklearn.feature_extraction.text import CountVectorizer vectorizer = CountVectorizer ( min_df = 1 ) corpus = [ 'This is the first document.' , 'This is the second second document.' , 'And the third one.' , 'Is this the first document?' , ] X = vectorizer . fit_transform ( corpus ) print vectorizer . get_feature_names () #[u'and', u'document', u'first', u'is', \\ #u'one', u'second', u'the', u'third', u'this'] for word , index in sorted ( vectorizer . vocabulary_ . items (), key = lambda x : x [ 1 ]): print \" %s \\t %d \" % ( word , index ) # and 0 # document 1 # first 2 # is 3 # one 4 # second 5 # the 6 # third 7 # this 8 for v in X : print v . toarray () # [[0 1 1 1 0 0 1 0 1]] # [[0 1 0 1 0 2 1 0 1]] # [[1 0 0 0 1 0 1 1 0]] # [[0 1 1 1 0 0 1 0 1]] print vectorizer . transform ([ 'Something completely new.' ]) . toarray () # [[0 0 0 0 0 0 0 0 0]] 上面代码中： 函数fit_transform()会在训练数据上训练一个Vectorizer，并将训练数据向量化，由于文本特征维度较高，这里是用稀疏矩阵保存 函数get_feature_names()得到特征名称的list, 特征名称为unicode字符串，而vectorizer的成员变量vocabulary_恰好是特征名称到特征索引的dict，这个索引就是get_feature_names()得到特征list的下标 函数transform()将新的文本向量化 通常情况下，每个特征权重会取tf-idf值，这个时候就应该使用 TfidfVectorizer from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer ( min_df = 1 , use_idf = True ) X = vectorizer . fit_transform ( corpus ) features = vectorizer . get_feature_names () for i , idf in enumerate ( vectorizer . idf_ ): print \" %s \\t %.5f \" % ( features [ i ] . encode ( 'utf-8' ), idf ) # and 1.91629 # document 1.22314 # first 1.51083 # is 1.22314 # one 1.91629 # second 1.91629 # the 1.00000 # third 1.91629 # this 1.22314 TfidfVectorizer成员函数与CountVectorizer一样，但是多了成员变量idf_, 为每个特征idf值构成的list。如果仅仅需要统计一份idf词表出来，也可以用TfidfVectorizer，idf计算公式为log((N+1)/(df+1))+1，分子分母都+1是假定增加一篇文档包含所有词。 其他说明 如果有自己的词表，相当于预先指定了特征，那么可以用 DictVectorizer 如果需要考虑短语或多词表达式，或者说考虑词之间的次序依赖关系，那么可以引入ngram模型，当ngram特征非常大的时候，需要考虑使用 HashingVectorizer 3. 分类实验 直接参考文章： Classification of text documents using sparse features 演示常见分类算法的效果，并且还使用卡方测试选取有效文本特征进行降维后再进行分类，运行结果如下： 相关参考 scikit-learn官网 特征选择","tags":"Tech","title":"sklearn使用笔记"},{"url":"/posts/2015/05/10/sha-men-san-ri-you.html","text":"从5月8日到5月10日，跟着豪华旅行团，在厦门开心地玩了3天。 第一天 下午坐动车去，落脚酒店,安顿下来后，去夜市逛了逛。 第二天 这一天包括了主要的行程：上午先在海边骑了自行车，然后去了炮台，下午则去了这次旅行的主要目标——鼓浪屿岛。 第三天 去了南普陀和厦门大学。因为赶车，比较匆忙地看了看。","tags":"Life","title":"厦门三日游"},{"url":"/posts/2015/01/24/jian-zhan-bei-wang.html","text":"一直想着搭建一个自己的网站，这次终于付诸行动了。参考了网上的许多资料，所以在这里就简单记录下自己的建站过程。 购买域名 从godaddy购买域名。也可用选择从万网和新网购买域名，这些网站还提供一条龙的服务，如提供主机，帮助备案等。 选择托管网站的主机 购买虚拟主机或者 VPS ，也可选择存储在github上，我选择的后者（以后有时间再买个主机试试）。如果要将网页放在github上，需要申请一个github账号，并创建一个与帐号同名的项目。具体参考 GitHub Pages 。 生成网页 Github推荐使用Jekyll构建自己的github pages。由于偏好python，我选择的pelican（鹈鹕）。Pelican是一个python包，使用pelican生成博客框架的过程如下： sudo apt-get install python-pip pip install pelican pip install markdown mkdir myblog cd myblog pelican-quickstart 在myblog/content目录下编写markdown文件：test.md,然后在myblog目录下执行如下命令： make html make serve 在myblog/output目录下会生成页面，同时在浏览器输入 localhost:8000 可以查看结果。 Markdown语法可参考 Markdown Basics ，Ubuntu-14.04下貌似默认安装了编辑器retext。 此外，需要加入如下几个功能： 选择博客主题。下载pelican主题项目进行配置。 git clone --recursive https://github.com/getpelican/pelican-themes.git cd pelican-themes pelican-themes -i ./pelican-themes/elegent 增加sitemap。下载pelican插件项目进行配置。 git clone https://github.com/getpelican/pelican-plugins cd pelican-plguins 增加评论。 disqus 提供了评论功能，注册账号即可获取一个shortname，将shortname加入pelicanconf.py,生成的页面中就会加入评论功能。 增加站长统计。可选择google或者百度的站长工具。如果选择gogole，在 Google Analytics 创建帐号，将追踪 ID 加入pelicanconf.py，生成页面中就会加入追踪功能。在 Google Webmasters 可查看追踪结果。 最后，配置文件myblog/pelicanconf.py基本如下： AUTHOR = u'lostfish' SITENAME = u\"lostfish\" SITEURL = 'http://tangke.me' THEME = 'elegent' DISQUS_SITENAME = 'disqus提供的shortname' GOOGLE_ANALYTICS = 'google提供的追踪id' PATH = 'content' #网页内容对应的markdown文件路径 ... # 设置生成页面存储路径为pots/年/月/日/{slug}.html (slug即文章标题拼音) USE_FOLDER_AS_CATEGORY = False ARTICLE_URL = 'posts/{date:%Y}/{date:%m}/{date:%d}/{slug}.html' ARTICLE_SAVE_AS = 'posts/{date:%Y}/{date:%m}/{date:%d}/{slug}.html' PAGE_URL = 'pages/{slug}.html' PAGE_SAVE_AS = 'pages/{slug}.html' YEAR_ARCHIVE_SAVE_AS = 'posts/{date:%Y}/index.html' MONTH_ARCHIVE_SAVE_AS = 'posts/{date:%Y}/{date:%m}/index.html' #站点地图插件配置 PLUGIN_PATHS = [\"pelican-plugins\"] PLUGINS = ['sitemap', 'extract_toc', 'tipue_search', 'liquid_tags.img', 'neighbors', 'latex', 'related_posts', 'share_post'] SITEMAP = { 'format': 'xml', 'priorities': { 'articles': 0.7, 'indexes': 0.5, 'pages': 0.3, }, 'changefreqs': { 'articles': 'monthly', 'indexes': 'daily', 'pages': 'monthly' } } 绑定域名 将自己购买的域名与github page绑定在一起需要3步： 在github pages项目下增加文件 CNAME ，在该文件中加入购买的域名。 在godaddy中将购买域名对应的域名解析服务器绑定到dnspod的服务器，可参考: Godaddy注册商域名修改 DNS 地址 。 注册dnspod账户，将自己购买的域名与github的服务器绑定,建立两条A类记指向192.30.252.153和192.30.252.154（这个 IP 可能会变化，参考github上 教程 ）。另外，这个配置最晚可能要等72小时之后才生效。 有用参考 Pelican官方文档 Pelican主题列表 主题elegent配置说明","tags":"Tech","title":"建站备忘"}]}