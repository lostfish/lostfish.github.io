{"pages":[{"url":"/posts/2016/08/27/eclatsuan-fa-shi-xian.html","text":"基本原理 Eclat是挖掘频繁项集的一种算法，相比较而言远，它远没有另外两种算法Apriori和 FP -Growth那么出名，一个直接的证明就是以前读书的时候课本都没有介绍。 不过这个算法也有它自身的特点，基本原理和Apriori算法很相似，也是通过频繁K项集连接生成频繁K+1项集，但是比Apriori速度快很多，因为引入了倒排机制，保留了每个频繁项集对应的所有记录。这样，在连接生成频繁K+1项集的过程中，两个频繁K项集对应记录集合求交，就得到了这个K+1项集对应的记录集合，这个集合的size就是K+1项集的出现频数，通过与支持度比较就可以判断新产生的K+1项集是不是频繁的。从这个过程可以看出，Eclat算法只需要扫描一次记录库，而Apriori算法每次判断候选K+1项集是不是频繁的时候都要扫描一次记录库，所以Eclat比Apriori快也就不奇怪了。不难看出，Eclat算法其实是用空间换取了时间，后续实验也证明了这个算法是比较耗费空间的。 举个例子（参考[1])： 有4条记录 tid item 1 A,B 2 B,C 3 A,C 4 A,B,C 设定最小支持度为2，第一边扫描记录库后，得到频繁1项集，其中每个item对应的记录集合也保留了，如下： item tids freq A 1,3,4 3 B 1,2,4 3 C 2,3,4 3 由频繁1项集连接生成候选2项集，都是频繁的： item tids freq A,B 1,4 2 A,C 3,4 2 B,C 2,4 2 由频繁2项集连接生成候选3项集，只有一个候选并且不是频繁的，算法结束： item tids freq A,B,C 4 1 注意 ：在最后一步中，只有A,B和A,C前缀相同，可以连接 实现思路 尝试用C++进行了基本的实现。这里总结几个要点： 所有item都转换为整数表示，项集用存储item的有序vector表示，这样连接生成K+1项集的时候方便快速比较前缀 每个频繁项集和记录集合的映射关系用map，记录集合用set（貌似比较耗费空间，可能用位图可以优化） 连接过程关键是比较前缀，比如有两个K项集，那么比较前k-1项是否相同，相同就把其中一个K项集最后一项追加到另一个最后，构成K+1项集，追加后要保证K+1项集是有序的。 从算法过程中还可以看出：频繁K项集的计算，需要把前面频繁1,2，.., k-1项集都计算后才能进行。此外，频繁1项集和频繁2项集的计算需要特殊处理，不用比较前缀。 具体实现参考代码： https://github.com/lostfish/eclat 实验结果 测试数据采用和[2]相同的mushroom.dat，该数据集下载地址为[3]，下载后需要去掉每行最后的空格。 机器配置为：Intel(R) Xeon(R) CPU E5606 @ 2.13GHz (8核，32G内存) 测试结果如下： 最小支持度 频繁项集数 运行时间 812(0.1) 574513 29.219 1218(0.15) 98575 4.593 1624(0.2) 53663 2.632 2031(0.25) 5545 0.603 当支持度为812时，挖掘的各频繁项集数目如下： valid_num : 8124 8124 uniq id num : 119 frequent 1 - itemset size : 56 frequent 2 - itemset size : 763 frequent 3 - itemset size : 4599 frequent 4 - itemset size : 16171 frequent 5 - itemset size : 38829 frequent 6 - itemset size : 69854 frequent 7 - itemset size : 98852 frequent 8 - itemset size : 111787 frequent 9 - itemset size : 100660 frequent 10 - itemset size : 71342 frequent 11 - itemset size : 39171 frequent 12 - itemset size : 16292 frequent 13 - itemset size : 4956 frequent 14 - itemset size : 1039 frequent 15 - itemset size : 134 frequent 16 - itemset size : 8 同时还观察了下空间消耗，占用内存达到了将近9G，比较惊人。当然这个数据集合本身频繁模式就比较多，平时应用中一般也不需要计算所有频繁项集，一般到频繁3项集或4项集就够了。另外算法本身也可以优化，比如记录集合用位图来表示。 相关参考 [1] https://zh.wikipedia.org/zh/%E5%85%B3%E8%81%94%E5% BC %8F%E8%A7%84%E5%88%99 [2] http://blog.csdn.net/yangliuy/article/details/7494983 [3] http://fimi.ua.ac.be/data/mushroom.dat","tags":"Tech","title":"Eclat算法实现"},{"url":"/posts/2016/07/22/pythondai-li-xia-zai.html","text":"当用同一个ip不断地去爬取一个网站的时候，很可能封掉，这时候就需要用到代理来分散请求。 分两种情况 代理设置了dns，调用函数crawl_page2() 代理没有设置dns，调用函数crawl_page()，这种情况稍微复杂点，先要获取url域名对应的ip地址，给http请求包加上dest_ip字段 代码如下 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 #!/usr/bin/env python # encoding:utf-8 import sys import time import urllib2 import socket import struct def get_dest_ip ( domain ): ip_addr = socket . gethostbyname ( domain ) #just for ipv4 #ip_addr = socket.getaddrinfo(domain, 'http')[0][4][0] #[(2, 1, 6, '', ('14.215.177.38', 80)), (2, 2, 17, '', ('14.215.177.38', 80)), (2, 1, 6, '', ('14.215.177.37', 80)), (2, 2, 17, '', ('14.215.177.37', 80))] uint32_binary_str = socket . inet_aton ( str ( ip_addr )) unpack_result = struct . unpack ( \"!I\" , uint32_binary_str ) ip_int = socket . htonl ( unpack_result [ 0 ]) return ip_int def crawl_page ( url , dest_ip , cur_proxy ): ''' ''' content = '' myheaders = { \"User-Agent\" : \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.2; .NET CLR 1.0.3705;)\" , \"Proxy-Connection\" : \"Keep-Alive\" , \"dest_ip\" : dest_ip } p = \"http:// %s \" % cur_proxy h = urllib2 . ProxyHandler ({ \"http\" : p }) o = urllib2 . build_opener ( h , urllib2 . HTTPHandler ) o . addheaders = myheaders . items () try : r = o . open ( url , timeout = 5 ) content = r . read () except urllib2 . HTTPError , e : print \"Error Code:\" , e . code if e . code == 404 : print \"No page: %s \" % url except urllib2 . URLError , e : print \"Error Reason:\" , e . reason except Exception as e : print \"Error\" , str ( e ) if len ( content ) > 10 : print \"Good \\t %s \" % p else : print \"Bad \\t %s \" % p return content def crawl_page2 ( url , cur_proxy = '' ): ''' get ''' # print \"-->crawl comment: %s\" % url # print \"-->cur_proxy: %s\" % cur_proxy myheaders = { \"User-Agent\" : \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.2; .NET CLR 1.0.3705;)\" } content = '' try : if cur_proxy : proxy_handler = urllib2 . ProxyHandler ({ 'http' : cur_proxy }) opener = urllib2 . build_opener ( proxy_handler ) f = opener . open ( url , timeout = 5 ) content = f . read () else : req = urllib2 . Request ( url , headers = myheaders ) f = urllib2 . urlopen ( req , timeout = 5 ) content = f . read () except urllib2 . HTTPError , e : print \"Error Code:\" , e . code if e . code == 404 : print \"No page: %s \" % url except urllib2 . URLError , e : print \"Error Reason:\" , e . reason except Exception as e : print \"Error\" , str ( e ) time . sleep ( 1 ) return content if __name__ == '__main__' : # open proxys proxy_list = open ( sys . argv [ 1 ]) . readlines () url = \"http://www.baidu.com\" host = \"www.baidu.com\" dest_ip = str ( get_dest_ip ( host )) for p in proxy_list : p = p . strip () n1 = len ( crawl_page ( url , dest_ip , p )) print \"crawl_page len:\" , n1 n2 = len ( crawl_page2 ( url , p )) print \"crawl_page2 len:\" , n2 测试输出 输入文件为两个代理ip，一个配置了dns，一个没有配置dns Good http://10.183.27.147:32810 crawl_page len: 10811 crawl_page2 len: 10811 Good http://10.184.16.44:32810 crawl_page len: 10811 Error Reason: timed out crawl_page2 len: 0 其他说明 获取dest_ip是访问的dns服务，并不是访问的原网站，而dns是带本地cache，所以频繁访问应该是没有问题的。 函数get_dest_ip()中调用了socket模块的相关函数: socket.gethostbyname() 获取域名对应ip，只支持ipv4，若需要支持ipv6，可使用函数socket.getaddrinfo() socket.inet_aton() 转换ip地址（192.168.1.10）为32位打包二进制字符串，只支持ipv4，若需要支持ipv6，可使用函数socket.inet_pton() socket.htonl() 将32位整数从主机字节序转换成网络字节序 参考 http://www.programgo.com/article/11342723643/ http://www.cnblogs.com/gala/archive/2011/09/22/2184801.html","tags":"Tech","title":"python代理下载"},{"url":"/posts/2016/07/03/qing-hai-xing.html","text":"六月是个好月份，适合旅行。 从22号到26号共五天的时间里，跟团去大青海体验了祖国西部的大好河山。 启程 傍晚时分，天气正好，带一丝燥热，跟随大队伍从深圳宝启程啦。 有点漫长的旅程，飞着飞着天就黑了。 到西宁的时候，已经是午夜。一下飞机就感受到了大西部的寒冷。后面更是感觉到了干燥和缺氧。 第一站 青海湖，美，辽阔，Relaxing！ 第二站 茶卡盐湖，白，天空之境，特别的美。 第三站 沙岛，还是青海湖，但是体验沙漠游玩项目：骑骆驼，骑摩托，滑沙。 最后一站 彩虹部落","tags":"Life","title":"青海行"},{"url":"/posts/2015/07/04/sklearnshi-yong-bi-ji.html","text":"sklearn全称为scikit-learn，是目前很流行的一个基于python的机器学习包，基本覆盖了常见的机器学习算法，如分类、回归、聚类、特征选择、模型选择以及数据预处理等任务对应的算法。文档和示例非常丰富，可视化展示也很方便，所以使用者众多，尤其是在 Kaggle 数据分析竞赛中被参赛者广泛使用。 关于该工具包的使用介绍网上已经非常多，所以这里只是整理和记录自己使用的一些心得。 初始入门 最好入门方法就是参考官网的 Tutorials sklearn集成了一些常用的数据集，以方便测试相关算法，封装为datasets模块，如导入分类数据集合iris和digits，导入回归数据集diabetes from sklearn import datasets iris = datasets . load_iris () digits = datasets . load_digits () diabetes = datasets . load_diabetes () 具体如何在这些数据集合应用相关算法，官网Tutorials有详细的介绍，下面重点讲下文本分类。 文本分类 1. 加载数据 数据集选取的是20newsgroups，该数据集包含20个新闻组约2万篇文档，加载方式也是通过sklearn.datasets模块，但是稍有不同，如下 from sklearn.datasets import fetch_20newsgroups categories = [ 'alt.atheism' , 'soc.religion.christian' , 'comp.graphics' , 'sci.med' ] twenty_train = fetch_20newsgroups ( subset = 'train' , categories = categories , shuffle = True , random_state = 42 ) 从上面可以看出，加载数据集实际上是调用函数fetch_20newsgroups() 在python命令行输入help(fetch_20newsgroups），可以查看相应参数说明，其中 subset: 指定加载训练集或测试集，或者两者，取值对应'train', ‘test', ‘all' data_home: 指定20newsgroups数据集所在路径，默认为 ‘~/scikit_learn_data'，Windows下对应为C:\\Users\\xxx\\scikit_learn_data categories: 指定要加载的类别list，默认为None，表示所有类别 shuffle： 是否要混洗数据 random_state： 混洗随机数的种子值 download_if_missing： 指定data_home下不存在数据集时是否下载，默认为True，表示下载 remove： 指定预处理文本的过滤策略，取值为元组 (‘headers', ‘footers', ‘quotes') 刚开始测试这个数据集的时候，老是等半天没有结果。于是看了下对应的源代码./site-packages/sklearn/datasets/twenty_newsgroups.py，发现第一次加载的时候会下载数据集到~/scikit_learn_data/20news_home，下载完成后会解压然后压缩生成cache文件，即~/scikit_learn_data/20news-bydate.pkz，以后每次加载就读取该文件了。 所以首次加载的时候需要有点耐心，大概等待7分钟左右吧（国内环境）。 当然也可以自己下载放到目录~/scikit_learn_data/20news_home，下载地址为： http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz （文件大小为13.7M） 2. 特征选择 最常见的就是词袋模型，每个词就是一个特征，特征权重为词频, 通过构建 CountVectorizer 来向量化每篇文档： from sklearn.feature_extraction.text import CountVectorizer vectorizer = CountVectorizer ( min_df = 1 ) corpus = [ 'This is the first document.' , 'This is the second second document.' , 'And the third one.' , 'Is this the first document?' , ] X = vectorizer . fit_transform ( corpus ) print vectorizer . get_feature_names () #[u'and', u'document', u'first', u'is', u'one', u'second', u'the', u'third', u'this'] for word , index in sorted ( vectorizer . vocabulary_ . items (), key = lambda x : x [ 1 ]): print \" %s \\t %d \" % ( word , index ) # and 0 # document 1 # first 2 # is 3 # one 4 # second 5 # the 6 # third 7 # this 8 for v in X : print v . toarray () # [[0 1 1 1 0 0 1 0 1]] # [[0 1 0 1 0 2 1 0 1]] # [[1 0 0 0 1 0 1 1 0]] # [[0 1 1 1 0 0 1 0 1]] print vectorizer . transform ([ 'Something completely new.' ]) . toarray () # [[0 0 0 0 0 0 0 0 0]] 上面代码中： 函数fit_transform()会在训练数据上训练一个Vectorizer，并将训练数据向量化，由于文本特征维度较高，这里是用稀疏矩阵保存 函数get_feature_names()得到特征名称的list, 特征名称为unicode字符串，而vectorizer的成员变量vocabulary_恰好是特征名称到特征索引的dict，这个索引就是get_feature_names()得到特征list的下标 函数transform()将新的文本向量化 通常情况下，每个特征权重会取tf-idf值，这个时候就应该使用 TfidfVectorizer from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer ( min_df = 1 , use_idf = True ) X = vectorizer . fit_transform ( corpus ) features = vectorizer . get_feature_names () for i , idf in enumerate ( vectorizer . idf_ ): print \" %s \\t %.5f \" % ( features [ i ] . encode ( 'utf-8' ), idf ) # and 1.91629 # document 1.22314 # first 1.51083 # is 1.22314 # one 1.91629 # second 1.91629 # the 1.00000 # third 1.91629 # this 1.22314 TfidfVectorizer成员函数与CountVectorizer一样，但是多了成员变量idf_, 为每个特征idf值构成的list。如果仅仅需要统计一份idf词表出来，也可以用TfidfVectorizer，idf计算公式为log((N+1)/(df+1))+1，分子分母都+1是假定增加一篇文档包含所有词。 其他说明 如果有自己的词表，相当于预先指定了特征，那么可以用 DictVectorizer 如果需要考虑短语或多词表达式，或者说考虑词之间的次序依赖关系，那么可以引入ngram模型，当ngram特征非常大的时候，需要考虑使用 HashingVectorizer 3. 分类实验 直接参考文章： Classification of text documents using sparse features 演示常见分类算法的效果，并且还使用卡方测试选取有效文本特征进行降维后再进行分类，运行结果如下： 相关参考 scikit-learn官网 特征选择","tags":"Tech","title":"sklearn使用笔记"},{"url":"/posts/2015/05/10/sha-men-san-ri-you.html","text":"从5月8日到5月10日，跟着豪华旅行团，在厦门开心地玩了3天。 第一天 下午坐动车去，落脚酒店,安顿下来后，去夜市逛了逛。 第二天 这一天包括了主要的行程：上午先在海边骑了自行车，然后去了炮台，下午则去了这次旅行的主要目标——鼓浪屿岛。 第三天 去了南普陀和厦门大学。因为赶车，比较匆忙地看了看。","tags":"Life","title":"厦门三日游"},{"url":"/posts/2015/01/24/jian-zhan-bei-wang.html","text":"一直想着搭建一个自己的网站，这次终于付诸行动了。参考了网上的许多资料，所以在这里就简单记录下自己的建站过程。 购买域名 从godaddy购买域名。也可用选择从万网和新网购买域名，这些网站还提供一条龙的服务，如提供主机，帮助备案等。 选择托管网站的主机 购买虚拟主机或者 VPS ，也可选择存储在github上，我选择的后者（以后有时间再买个主机试试）。如果要将网页放在github上，需要申请一个github账号，并创建一个与帐号同名的项目。具体参考 GitHub Pages 。 生成网页 Github推荐使用Jekyll构建自己的github pages。由于偏好python，我选择的pelican（鹈鹕）。Pelican是一个python包，使用pelican生成博客框架的过程如下： sudo apt-get install python-pip pip install pelican pip install markdown mkdir myblog cd myblog pelican-quickstart 在myblog/content目录下编写markdown文件：test.md,然后在myblog目录下执行如下命令： make html make serve 在myblog/output目录下会生成页面，同时在浏览器输入 localhost:8000 可以查看结果。 Markdown语法可参考 Markdown Basics ，Ubuntu-14.04下貌似默认安装了编辑器retext。 此外，需要加入如下几个功能： 选择博客主题。下载pelican主题项目进行配置。 git clone --recursive https://github.com/getpelican/pelican-themes.git cd pelican-themes pelican-themes -i ./pelican-themes/elegent 增加sitemap。下载pelican插件项目进行配置。 git clone https://github.com/getpelican/pelican-plugins cd pelican-plguins 增加评论。 disqus 提供了评论功能，注册账号即可获取一个shortname，将shortname加入pelicanconf.py,生成的页面中就会加入评论功能。 增加站长统计。可选择google或者百度的站长工具。如果选择gogole，在 Google Analytics 创建帐号，将追踪 ID 加入pelicanconf.py，生成页面中就会加入追踪功能。在 Google Webmasters 可查看追踪结果。 最后，配置文件myblog/pelicanconf.py基本如下： AUTHOR = u'lostfish' SITENAME = u\"lostfish\" SITEURL = 'http://tangke.me' THEME = 'elegent' DISQUS_SITENAME = 'disqus提供的shortname' GOOGLE_ANALYTICS = 'google提供的追踪id' PATH = 'content' #网页内容对应的markdown文件路径 ... # 设置生成页面存储路径为pots/年/月/日/{slug}.html (slug即文章标题拼音) USE_FOLDER_AS_CATEGORY = False ARTICLE_URL = 'posts/{date:%Y}/{date:%m}/{date:%d}/{slug}.html' ARTICLE_SAVE_AS = 'posts/{date:%Y}/{date:%m}/{date:%d}/{slug}.html' PAGE_URL = 'pages/{slug}.html' PAGE_SAVE_AS = 'pages/{slug}.html' YEAR_ARCHIVE_SAVE_AS = 'posts/{date:%Y}/index.html' MONTH_ARCHIVE_SAVE_AS = 'posts/{date:%Y}/{date:%m}/index.html' #站点地图插件配置 PLUGIN_PATHS = [\"pelican-plugins\"] PLUGINS = ['sitemap', 'extract_toc', 'tipue_search', 'liquid_tags.img', 'neighbors', 'latex', 'related_posts', 'share_post'] SITEMAP = { 'format': 'xml', 'priorities': { 'articles': 0.7, 'indexes': 0.5, 'pages': 0.3, }, 'changefreqs': { 'articles': 'monthly', 'indexes': 'daily', 'pages': 'monthly' } } 绑定域名 将自己购买的域名与github page绑定在一起需要3步： 在github pages项目下增加文件 CNAME ，在该文件中加入购买的域名。 在godaddy中将购买域名对应的域名解析服务器绑定到dnspod的服务器，可参考: Godaddy注册商域名修改 DNS 地址 。 注册dnspod账户，将自己购买的域名与github的服务器绑定,建立两条A类记指向192.30.252.153和192.30.252.154（这个 IP 可能会变化，参考github上 教程 ）。另外，这个配置最晚可能要等72小时之后才生效。 有用参考 Pelican官方文档 Pelican主题列表 主题elegent配置说明","tags":"Tech","title":"建站备忘"}]}